import cv2
import mediapipe as mp
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
import pickle
import threading
import speech_recognition as sr
import queue
import time

# Recognizer & Queue ƒë·ªÉ nghe song song
recognizer = sr.Recognizer()
voice_queue = queue.Queue()
ENABLE_VOICE = False
def listen_for_voice():
    while True:
        with sr.Microphone() as source:
            try:
                print("üé§ ƒêang l·∫Øng nghe... (n√≥i: ƒë√∫ng / sai / tho√°t / t√© / ƒë·ª©ng / ng·ªìi)")
                audio = recognizer.listen(source, phrase_time_limit=2)
                text = recognizer.recognize_google(audio, language="vi-VN").lower()
                print("üëÇ B·∫°n n√≥i:", text)

                if "ƒë√∫ng" in text:
                    voice_queue.put("d")
                elif "sai" in text:
                    voice_queue.put("s")
                elif "tho√°t" in text or "thoat" in text:
                    voice_queue.put("a")
                elif "t√©" in text or "ng√£" in text or "nga" in text or "fall" in text:
                    voice_queue.put("f")
                elif "ƒë·ª©ng" in text or "dung" in text:
                    voice_queue.put("u")
                elif "ng·ªìi" in text or "ngoi" in text:
                    voice_queue.put("n")
            except Exception as e:
                # T√πy ch·ªçn: in l·ªói ra n·∫øu c·∫ßn debug
                # print(f"L·ªói gi·ªçng n√≥i: {e}")
                continue

# Kh·ªüi ƒë·ªông lu·ªìng nh·∫≠n gi·ªçng n√≥i
if ENABLE_VOICE:
    listener_thread = threading.Thread(target=listen_for_voice, daemon=True)
    listener_thread.start()

# Mediapipe pose setup
mp_pose = mp.solutions.pose
pose = mp_pose.Pose()
mp_drawing = mp.solutions.drawing_utils

SELECTED_LANDMARKS = list(range(33))  # D√πng to√†n b·ªô pose 33 ƒëi·ªÉm

UPPER_BODY_CONNECTIONS = [
    (0, 11), (0, 12), (11, 13), (13, 15), (12, 14), (14, 16),
    (11, 12), (11, 23), (12, 24), (23, 24)
]

data, labels = [], []
cap = cv2.VideoCapture(0)
print("‚û°Ô∏è Nh·∫•n 'd'=ƒê√öNG, 's'=SAI, 'a'=THO√ÅT ho·∫∑c n√≥i t∆∞∆°ng ·ª©ng")

while True:
    ret, frame = cap.read()
    if not ret:
        continue

    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    res = pose.process(img_rgb)

    action = None

    if res.pose_landmarks:
        landmarks = res.pose_landmarks.landmark
        h, w, _ = frame.shape

        # V·∫Ω c√°c k·∫øt n·ªëi x∆∞∆°ng tr√™n c∆° th·ªÉ
        for start, end in UPPER_BODY_CONNECTIONS:
            x1, y1 = int(landmarks[start].x * w), int(landmarks[start].y * h)
            x2, y2 = int(landmarks[end].x * w), int(landmarks[end].y * h)
            cv2.line(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)

        # V·∫Ω c√°c ƒëi·ªÉm kh·ªõp ƒë√£ ch·ªçn
        for idx in SELECTED_LANDMARKS:
            x, y = int(landmarks[idx].x * w), int(landmarks[idx].y * h)
            cv2.circle(frame, (x, y), 6, (0, 0, 255), -1)

        # L∆∞u keypoints
        keypoints = []
        for idx in SELECTED_LANDMARKS:
            lm = landmarks[idx]
            keypoints.extend([lm.x, lm.y, lm.z])

        # H∆∞·ªõng d·∫´n hi·ªÉn th·ªã
        cv2.putText(frame, "N√≥i/nh·∫•n: [d]=ƒê√∫ng, [s]=Sai, [f]=T√©, [u]=ƒê·ª©ng, [n]=Ng·ªìi, [a]=Tho√°t",
                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)

        # Nh·∫≠n t·ª´ b√†n ph√≠m
        key = cv2.waitKey(10)
        if key in [ord("d"), ord("s"), ord("a"), ord("f"), ord("u"), ord("n")]:
            action = chr(key)
        elif not voice_queue.empty():
            action = voice_queue.get()

        # X·ª≠ l√Ω h√†nh ƒë·ªông
        if action == "d":
            data.append(keypoints)
            labels.append(1)
            print("‚úÖ ƒê√É L∆ØU: ƒê√öNG")
        elif action == "s":
            data.append(keypoints)
            labels.append(0)
            print("‚ö†Ô∏è ƒê√É L∆ØU: SAI")
        elif action == "f":
            data.append(keypoints)
            labels.append(2)
            print("üö® ƒê√É L∆ØU: T√â NG√É")
        elif action == "u":
            data.append(keypoints)
            labels.append(3)
            print("üßç‚Äç‚ôÇÔ∏è ƒê√É L∆ØU: ƒê·ª®NG")
        elif action == "n":
            data.append(keypoints)
            labels.append(4)
            print("ü™ë ƒê√É L∆ØU: NG·ªíI")
        elif action == "a":
            print("üëã Tho√°t ch∆∞∆°ng tr√¨nh hu·∫•n luy·ªán.")
            break

    # Hi·ªÉn th·ªã frame
    cv2.imshow("Train Pose (Voice + Key)", frame)

cap.release()
cv2.destroyAllWindows()

if not data:
    print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u.")
    exit()

df = pd.DataFrame(data)
df['label'] = labels
df.to_csv("pose_data.csv", index=False)

clf = RandomForestClassifier(n_estimators=100)
clf.fit(data, labels)
pickle.dump(clf, open("pose_model.pkl", "wb"))

print("‚úÖ ƒê√£ l∆∞u th√†nh c√¥ng: pose_data.csv & pose_model.pkl")
